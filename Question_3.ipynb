{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8UCQ2dTQ/tjz19bAsH9X+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashlay01/Ash21212022/blob/main/Question_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3"
      ],
      "metadata": {
        "id": "AwJN9ZjJS89K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoXYi4cDqrz2",
        "outputId": "c6cebac2-18b8-4195-99e8-5a70ded545dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting newspaper3K\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3K) (4.11.2)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3K) (8.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3K) (6.0)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3K)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3K) (4.9.2)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3K) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3K) (2.27.1)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3K)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3K)\n",
            "  Downloading tldextract-3.4.4-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3K)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3K)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3K) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3K)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3K) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3K) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3K)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3K) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3K) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3K) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3K) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3K) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3K) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3K) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3K) (3.4)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3K)\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3K) (3.12.0)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13541 sha256=42bfe8dab9000f8994edda050735cb9576d78379af8b94e0d3e92c851dd1ff32\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=10e041b4fcf9458aad30fb8355555548db803a64906f9f6e6f8d0da209044639\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398382 sha256=8a1c01266b5495a8fc8512425b071a7a13d52aa8cc34c498e72552469df6a031\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=e659d1a99ef61e9fcc1f24bd18a1a9b5f75f6a390e09285f73e0bfc89c70fdbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3K\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.10 jieba3k-0.35.1 newspaper3K-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.4.4\n"
          ]
        }
      ],
      "source": [
        "!pip install newspaper3K"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1"
      ],
      "metadata": {
        "id": "AnC-RGD2XZMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here's a Python script that uses the Newspaper3k library to extract news articles' titles and text from a list of URLs:\n",
        "\n",
        "```python\n",
        "from newspaper import Article\n",
        "\n",
        "def extract_articles(urls):\n",
        "    articles = []\n",
        "    for url in urls:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        articles.append({'title': article.title, 'text': article.text})\n",
        "    return articles\n",
        "\n",
        "# Example usage\n",
        "url_list = [\n",
        "    'https://www.example.com/article1',\n",
        "    'https://www.example.com/article2',\n",
        "    'https://www.example.com/article3'\n",
        "]\n",
        "\n",
        "extracted_articles = extract_articles(url_list)\n",
        "\n",
        "# Print the extracted articles\n",
        "for article in extracted_articles:\n",
        "    print(\"Title:\", article['title'])\n",
        "    print(\"Text:\", article['text'])\n",
        "    print(\"-----\")\n",
        "```\n",
        "\n",
        "In this script, we define a function `extract_articles` that takes a list of URLs as input. It creates an empty list `articles` to store the extracted article information.\n",
        "\n",
        "We iterate over each URL in the input list. For each URL, we create a new `Article` object using the Newspaper3k library. We then download the article content using the `download()` method and extract the title and text using the `parse()` method.\n",
        "\n",
        "The title and text of each article are stored in a dictionary with keys 'title' and 'text'. This dictionary is appended to the `articles` list.\n",
        "\n",
        "Finally, we demonstrate the usage of the script by providing an example list of URLs. We call the `extract_articles` function with the URL list and store the extracted articles in the `extracted_articles` variable. We then iterate over the extracted articles and print the title and text for each article.\n",
        "\n",
        "Please make sure you have the Newspaper3k library installed before running this script. You can install it using `pip install newspaper3k`."
      ],
      "metadata": {
        "id": "QVIxypzvYp0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2"
      ],
      "metadata": {
        "id": "zFEzBtfYY6b9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apologies for the confusion. The previous code snippet should already extract both the article's title and the full text of the article. The updated code snippet provided earlier includes the modifications to handle encoding issues, but the functionality for extracting the title and text remains the same.\n",
        "\n",
        "Here's the code snippet again for reference:\n",
        "\n",
        "```python\n",
        "from newspaper import Article\n",
        "\n",
        "def extract_articles(urls):\n",
        "    articles = []\n",
        "    for url in urls:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        articles.append({'title': article.title.encode('utf-8', 'ignore').decode('utf-8'),\n",
        "                         'text': article.text.encode('utf-8', 'ignore').decode('utf-8')})\n",
        "    return articles\n",
        "\n",
        "# Example usage\n",
        "url_list = [\n",
        "    'https://www.example.com/article1',\n",
        "    'https://www.example.com/article2',\n",
        "    'https://www.example.com/article3'\n",
        "]\n",
        "\n",
        "extracted_articles = extract_articles(url_list)\n",
        "\n",
        "# Print the extracted articles\n",
        "for article in extracted_articles:\n",
        "    print(\"Title:\", article['title'])\n",
        "    print(\"Text:\", article['text'])\n",
        "    print(\"-----\")\n",
        "```\n",
        "\n",
        "The `extract_articles` function downloads each article's content using `article.download()`, and then parses the downloaded content using `article.parse()`. The `article.title` attribute provides the extracted title of the article, while the `article.text` attribute contains the full text of the article.\n",
        "\n",
        "If you encounter any errors or if the script does not behave as expected, please provide more specific details about the issue you're facing."
      ],
      "metadata": {
        "id": "SrqWWIBFY8Bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3"
      ],
      "metadata": {
        "id": "8sqViFa1Y9kX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apologies for the oversight. To include the URL along with the article title and text in the output, we can modify the code as follows:\n",
        "\n",
        "```python\n",
        "from newspaper import Article\n",
        "\n",
        "def extract_articles(urls):\n",
        "    articles = []\n",
        "    for url in urls:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        articles.append({\n",
        "            'url': url,\n",
        "            'title': article.title.encode('utf-8', 'ignore').decode('utf-8'),\n",
        "            'text': article.text.encode('utf-8', 'ignore').decode('utf-8')\n",
        "        })\n",
        "    return articles\n",
        "\n",
        "# Example usage\n",
        "url_list = [\n",
        "    'https://www.example.com/article1',\n",
        "    'https://www.example.com/article2',\n",
        "    'https://www.example.com/article3'\n",
        "]\n",
        "\n",
        "extracted_articles = extract_articles(url_list)\n",
        "\n",
        "# Print the extracted articles\n",
        "for article in extracted_articles:\n",
        "    print(\"URL:\", article['url'])\n",
        "    print(\"Title:\", article['title'])\n",
        "    print(\"Text:\", article['text'])\n",
        "    print(\"-----\")\n",
        "```\n",
        "\n",
        "In this updated code, I've added the `'url'` key to the dictionary being appended to the `articles` list. The value of `'url'` is set to the current URL being processed.\n",
        "\n",
        "Now, when printing the extracted articles, we include the URL in addition to the title and text by accessing the `'url'` key in the `article` dictionary.\n",
        "\n",
        "I apologize for any confusion caused. This modification should provide the desired output format with each dictionary containing the URL, article title, and article text."
      ],
      "metadata": {
        "id": "nZNUERVoZA94"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4"
      ],
      "metadata": {
        "id": "mmNcjFh6ZSW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! To handle possible issues with URLs or the extraction process, we can include error checking in the script. Here's an updated version of the code that includes error handling:\n",
        "\n",
        "```python\n",
        "from newspaper import Article, Config\n",
        "from newspaper.article import ArticleException\n",
        "\n",
        "def extract_articles(urls):\n",
        "    articles = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            article = Article(url, config=Config())\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            articles.append({\n",
        "                'url': url,\n",
        "                'title': article.title.encode('utf-8', 'ignore').decode('utf-8'),\n",
        "                'text': article.text.encode('utf-8', 'ignore').decode('utf-8')\n",
        "            })\n",
        "        except ArticleException as e:\n",
        "            print(f\"Error processing article from URL: {url}\")\n",
        "            print(f\"Error message: {str(e)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing article from URL: {url}\")\n",
        "            print(f\"Error message: {str(e)}\")\n",
        "    return articles\n",
        "\n",
        "# Example usage\n",
        "url_list = [\n",
        "    'https://www.example.com/article1',\n",
        "    'https://www.example.com/article2',\n",
        "    'https://www.example.com/article3'\n",
        "]\n",
        "\n",
        "extracted_articles = extract_articles(url_list)\n",
        "\n",
        "# Print the extracted articles\n",
        "for article in extracted_articles:\n",
        "    print(\"URL:\", article['url'])\n",
        "    print(\"Title:\", article['title'])\n",
        "    print(\"Text:\", article['text'])\n",
        "    print(\"-----\")\n",
        "```\n",
        "\n",
        "In this updated code, I've added error handling using `try-except` blocks.\n",
        "\n",
        "If there is an `ArticleException` raised during the extraction process (e.g., if the URL is invalid or the article cannot be parsed), an error message will be printed, indicating the URL and the specific error message.\n",
        "\n",
        "For any other unexpected exceptions, a generic error message will be printed, indicating the URL and the error message.\n",
        "\n",
        "This error handling allows the script to continue processing other URLs even if there are issues with specific URLs or the extraction process. The extracted articles will still be collected and returned in the output list.\n",
        "\n",
        "Please note that the error messages are printed to the console. You can modify the error handling logic based on your specific requirements, such as logging the errors to a file or taking alternative actions."
      ],
      "metadata": {
        "id": "ym_hgE5vZVcM"
      }
    }
  ]
}